quanti.var <- get_famd_var(res.famd, "quanti.var")
fviz_famd_var(res.famd, "quali.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
quanti.var <- get_famd_var(res.famd, "quanti.var")
fviz_famd_var(res.famd, "quanti.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE)
fviz_famd_var(res.famd, repel = TRUE)
fviz_famd_var(res.famd, "quali.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_screeplot(res.famd)
df <- read.csv('C:\\Users\\Alex\\Documents\\jobs\\tvsquared\\assesment\\r_df.csv')
df <- df[,-c(1,2)]
df['Location.ID'] <- as.factor(df$Location.ID)
df['Month'] <- as.factor(df$Month)
df['Weekday'] <- as.factor(df$Weekday)
library(FactoMineR)
library(factoextra)
res.famd <- FAMD(df, graph = TRUE)
#all variables
fviz_famd_var(res.famd, repel = TRUE)
View(df)
df <- read.csv('C:\\Users\\Alex\\Documents\\jobs\\tvsquared\\assesment\\r_df.csv')
df <- df[,-c(1,2)]
df['Location.ID'] <- as.factor(df$Location.ID)
df['Month'] <- as.factor(df$Month)
df['Weekday'] <- as.factor(df$Weekday)
library(FactoMineR)
library(factoextra)
res.famd <- FAMD(df, graph = TRUE)
res.famd <- FAMD(df, graph = FALSE)
res.famd
quanti.var <- get_famd_var(res.famd, "quanti.var")
fviz_famd_var(res.famd, "quanti.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE)
#quantititive variables
quanti.var <- get_famd_var(res.famd, "quanti.var")
fviz_famd_var(res.famd, "quanti.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE)
fviz_famd_var(res.famd, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE)
#qualititive variables
quali.var <- get_famd_var(res.famd, "quali.var")
fviz_famd_var(res.famd, "quali.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
quali.var <- get_famd_var(res.famd, "quali.var")
quali.var
fviz_famd_var(res.famd, "quali.var", col.var = "contrib")
fviz_famd_var(res.famd, repel = TRUE)
res.famd <- FAMD(df, graph = TRUE)
quali.var <- get_famd_var(res.famd, "quali.var")
fviz_famd_var(res.famd, "quali.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_hmfa_quali_biplot(res.famd)
View(df)
str(df)
?fviz_famd_var
fviz_famd_var(res.famd, c("quanti.var", "quali.var"), col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_famd_var(res.famd, choice=c("quanti.var", "quali.var"), col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_famd_var(res.famd, choice="quali.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
row.names(df)
length(row.names(df))
length(unique(row.names(df)))
fviz_famd_var(res.famd, choice="quali.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
quali.var
quali.var$cos2
quali.var
quali.coord
quali.var$coord
quali.var
quali.var$contrib
quanti.var$cos2
res.famd <- FAMD(df, graph = TRUE)
fviz_famd_var(res.famd, repel = TRUE)
fviz_famd_var(res.famd, repel = FALSE)
fviz_famd_var(res.famd, repel = TRUE)
fviz_famd_var(res.famd, "quanti.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE)
str(df)
quali.var <- get_famd_var(res.famd, "quali.var")
fviz_famd_var(res.famd, choice="quali.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel=TRUE)
fviz_famd_var(res.famd, repel = TRUE)
df <- read.csv('C:\\Users\\Alex\\Documents\\jobs\\tvsquared\\assesment\\r_df.csv')
df <- df[,-c(1,2)]
df['Location.ID'] <- as.factor(df$Location.ID)
df['Month'] <- as.factor(df$Month)
df['Weekday'] <- as.factor(df$Weekday)
library(FactoMineR)
library(factoextra)
res.famd <- FAMD(df, graph = FALSE)
#all variables - most ueful plot so far
fviz_famd_var(res.famd, repel = TRUE)
#scree plot
#fviz_screeplot(res.famd)
#quantititive variables
quanti.var <- get_famd_var(res.famd, "quanti.var")
fviz_famd_var(res.famd, "quanti.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE)
#qualititive variables
#quali.var <- get_famd_var(res.famd, "quali.var")
#fviz_famd_var(res.famd, choice="quali.var", col.var = "contrib",
#              gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
#eig.val <- get_eigenvalue(res.famd)
quali.var <- get_famd_var(res.famd, "quali.var")
install.packages("corrplot")
corrplot(quali.var$cos2)
library(corrplot)
corrplot(quali.var$cos2)
fviz_famd_var(res.famd, choice="quali.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
# Chunk 1
library(dplyr)
library(rjson)
library(caret)
library(randomForest)
library(ROCR)
library(dummies)
#read in the train and test datasets and combine
df1 <- read.csv('dota_dataset\\dota2Train.csv')
df2 <- read.csv('dota_dataset\\dota2Test.csv')
names(df2) <- names(df1)
#combine the original datasets to a single dataframe
df <- rbind(df1,df2)
rm(df1,df2)
# Chunk 3
#record number of rows and columns
paste('number of columns =' , ncol(df) , '. number of rows =', nrow(df))
# Chunk 6
#check for missing values (there are none)
sum(complete.cases(df))==nrow(df)
#check there are equal number of heroes present in each team (hero values for all rows should sum to zero)
sum((rowSums(df[5:ncol(df)]))) == 0
# Chunk 7
df[sapply(df, is.numeric)] <- lapply(df[sapply(df, is.numeric)], as.factor)
# Chunk 8
#add column names to the first few columns
names(df)[1:4] <- c('won','cluster_id','game_mode','game_type')
#populate the other column names
#load in json file and use it to populate column names
hero_names <- fromJSON(file ="dota_dataset\\heroes.json")
for ( i in c(1:length(hero_names$heroes))){
names(df)[i+4] <- hero_names$heroes[[i]]$name
}
# Chunk 9
#drop unwanted columns
df <- df%>%
select(-c(lina, cluster_id , game_mode , game_type))
# Chunk 11
library(ggplot2)
p <- ggplot(iris, aes(x=Petal.Length,
y = Petal.Width,color=Species) )
p <- p+ geom_point(aes(shape=Species))
p <- p + xlab('Petal Length')
p <- p + ylab('Petal Width')
p <- p + theme_bw()
p
# Chunk 12
set.seed(57)
# Split into train test and validation sets
index <- sample(c(1:3), size = nrow(df), replace = TRUE, prob = c(.6, .2, .2))
df_train <- df[index == 1,]
df_test <- df[index == 2,]
df_valid <- df[index == 3,]
# Chunk 13
num_attributes <- ncol(df_train)
#build a parameter grid
# Random forest
mtry <- as.integer(c(num_attributes * 0.25, num_attributes / 3, num_attributes * 0.5, num_attributes * 0.8))
nodesize <- c( 3, 5, 20)
ntree <- c(200)
PG<- as.data.frame(expand.grid(mtry=mtry, nodesize=nodesize,
ntree=ntree, stringsAsFactors=F))
# Chunk 14
for (i in c(1:nrow(PG))){
mtry <- PG[i, "mtry"]
nodesize <- PG[i, "nodesize"]
ntree <- PG[i, "ntree"]
temp_model <- randomForest(formula(df_train) , data=df_train, mtry=mtry, ntree=ntree, nodesize=nodesize)
predictions <- predict(temp_model , df_valid)
cm<- confusionMatrix(predictions,df_valid$won)
accuracy <- cm$overall['Accuracy']
PG[i,'Accuracy'] <- accuracy
print(i)
}
# Chunk 15
#assess best model on test set
PG <- PG%>%
arrange(desc(Accuracy))
mtry <- PG[1, "mtry"]
nodesize <- PG[1, "nodesize"]
ntree <- PG[1, "ntree"]
best_model <- randomForest(formula(df_train) , data=df_train, mtry=mtry, ntree=ntree, nodesize=nodesize , importance=T)
predictions <- predict(temp_model , df_valid)
confusionMatrix(predictions,df_valid$won)
setwd("~/GitHub/streaming-analysis/twitter-fracking")
library(dplyr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(ggplot2)
library(gridExtra)
library(graph)
library(Rgraphviz)
library(RTextTools)
df <- read.csv("fracking_dataset.csv")
#function to build text corpus for analysis
buildCorpus <- function(someText){
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(someText))
# I had to add this line to make the code work
# For windows, it may not be an issue
myCorpus <- tm_map(myCorpus,
content_transformer(function(x) iconv(x, to='UTF-8',sub='byte')))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) {
gsub("http[[:alnum:]]*", "", x)
}
myCorpus <- tm_map(myCorpus, removeURL)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL)) #??
# add stopwords
myStopwords <- c(stopwords("english"))
#remove words CONTAINING certain terms
myCorpus <- tm_map(myCorpus, content_transformer(gsub), pattern = "*frack*|*ing*", replacement = "")
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
#
myCorpus <- tm_map(myCorpus, stripWhitespace)
# Return the text corpus
return(myCorpus)
}
#function to build term document matrix
make_tdm <- function(text_column,minfreq=1){
text_column <- iconv(text_column, 'UTF-8', 'ASCII')
corpus <- buildCorpus(text_column)
# keep for later
corpus_copy <- corpus
# stem words
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus,control=list(bounds = list(global = c(minfreq,Inf))))
dtm <- DocumentTermMatrix(corpus,control=list(bounds = list(global = c(minfreq,Inf))))
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
return(list(corpus=corpus,freq_table=d,tdm=tdm , dtm=dtm))
}
#make corpuses and tdms from the tweets
all_tweets <- make_tdm(df$X.2)
#function to make wordclouds
make_wordcloud <- function(freq_tab){
wordcloud(words = freq_tab$word, freq = freq_tab$freq, min.freq = 5,max.words=2000, random.order=FALSE, rot.per=0.2,colors=brewer.pal(8, "Dark2"))
}
#make wordclouds from the frequency tables
#make_wordcloud(all_tweets$freq_tab)
plot_wordcount <- function(freq_tab,title_text){
ggplot(head(freq_tab,20), aes(x=word,y=freq))+
geom_bar(stat="identity")+
labs(title=title_text)+
theme(axis.text.x = element_text(angle = 30, hjust = 1))
}
#list most frequent words in each list of tweets
plot_wordcount(all_tweets$freq_table,"most common words for all tweets")
#plot word associations
plot(all_tweets$tdm, term = findFreqTerms(all_tweets$tdm, lowfreq = 70), corThreshold = 0.1,
weighting = T, main="word association for all tweets")
#text classifier
#######
#we already have functions to build the document term matrix above so lets use those
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$X.2,15)
dtm<-dtm$dtm
#calculate distances
m  <- as.matrix(dtm)
m<-m[sample(nrow(m),size=5000,replace=FALSE),] #sample rows to prevent huge distance matrix
distMatrix <- dist(m, method="euclidean")
num_clusters <- 5
#perform hierachical clustering
hc <- hclust(distMatrix, "ward.D")
clustering <- cutree(hc, num_clusters)
test<-as.data.frame(cbind(m,clustering))
for (i in c(1:num_clusters)){
tryCatch({
cluster_dtm <- filter(test,clustering==as.character(i))%>%
select(-clustering)
m <- as.matrix(cluster_dtm)
v <- sort(colSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
make_wordcloud(d)
},
error = function(err){print("empty cluster")}
)
}
table(test$clustering)
i<-1
tryCatch({
cluster_dtm <- filter(test,clustering==as.character(i))%>%
select(-clustering)
m <- as.matrix(cluster_dtm)
v <- sort(colSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
make_wordcloud(d)
},
error = function(err){print("empty cluster")}
)
View(d)
View(cluster_dtm)
#we already have functions to build the document term matrix above so lets use those
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$X.2,15)
dtm<-dtm$dtm
#calculate distances
m  <- as.matrix(dtm)
m<-m[sample(nrow(m),size=5000,replace=FALSE),] #sample rows to prevent huge distance matrix
distMatrix <- dist(m, method="euclidean")
num_clusters <- 5
#perform hierachical clustering
hc <- hclust(distMatrix, "ward.D")
clustering <- cutree(hc, num_clusters)
test<-as.data.frame(cbind(m,clustering))
for (i in c(1:num_clusters)){
tryCatch({
cluster_dtm <- filter(test,clustering==as.character(i))%>%
select(-clustering)
m <- as.matrix(cluster_dtm)
v <- sort(colSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
make_wordcloud(d)
},
error = function(err){print("empty cluster")}
)
}
table(test$clustering)
View(d)
#we already have functions to build the document term matrix above so lets use those
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$X.2,15)
dtm<-dtm$dtm
#calculate distances
m  <- as.matrix(dtm)
m<-m[sample(nrow(m),size=2000,replace=FALSE),] #sample rows to prevent huge distance matrix
distMatrix <- dist(m, method="euclidean")
num_clusters <- 5
#perform hierachical clustering
hc <- hclust(distMatrix, "ward.D")
clustering <- cutree(hc, num_clusters)
test<-as.data.frame(cbind(m,clustering))
for (i in c(1:num_clusters)){
tryCatch({
cluster_dtm <- filter(test,clustering==as.character(i))%>%
select(-clustering)
m <- as.matrix(cluster_dtm)
v <- sort(colSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
make_wordcloud(d)
},
error = function(err){print("empty cluster")}
)
}
table(test$clustering)
View(d)
library(dplyr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(ggplot2)
library(gridExtra)
library(graph)
library(Rgraphviz)
library(RTextTools)
df <- read.csv("fracking_dataset.csv")
#function to build text corpus for analysis
buildCorpus <- function(someText){
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(someText))
# I had to add this line to make the code work
# For windows, it may not be an issue
myCorpus <- tm_map(myCorpus,
content_transformer(function(x) iconv(x, to='UTF-8',sub='byte')))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) {
gsub("http[[:alnum:]]*", "", x)
}
myCorpus <- tm_map(myCorpus, removeURL)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL)) #??
# add stopwords
myStopwords <- c(stopwords("english"))
#remove words CONTAINING certain terms
myCorpus <- tm_map(myCorpus, content_transformer(gsub), pattern = "*frack*|*ing*", replacement = "")
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
#
myCorpus <- tm_map(myCorpus, stripWhitespace)
# Return the text corpus
return(myCorpus)
}
#function to build term document matrix
make_tdm <- function(text_column,minfreq=1){
text_column <- iconv(text_column, 'UTF-8', 'ASCII')
corpus <- buildCorpus(text_column)
# keep for later
corpus_copy <- corpus
# stem words
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus,control=list(bounds = list(global = c(minfreq,Inf))))
dtm <- DocumentTermMatrix(corpus,control=list(bounds = list(global = c(minfreq,Inf))))
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
return(list(corpus=corpus,freq_table=d,tdm=tdm , dtm=dtm))
}
#make corpuses and tdms from the tweets
all_tweets <- make_tdm(df$X.2)
#function to make wordclouds
make_wordcloud <- function(freq_tab){
wordcloud(words = freq_tab$word, freq = freq_tab$freq, min.freq = 5,max.words=2000, random.order=FALSE, rot.per=0.2,colors=brewer.pal(8, "Dark2"))
}
#make wordclouds from the frequency tables
#make_wordcloud(all_tweets$freq_tab)
plot_wordcount <- function(freq_tab,title_text){
ggplot(head(freq_tab,20), aes(x=word,y=freq))+
geom_bar(stat="identity")+
labs(title=title_text)+
theme(axis.text.x = element_text(angle = 30, hjust = 1))
}
#list most frequent words in each list of tweets
plot_wordcount(all_tweets$freq_table,"most common words for all tweets")
#plot word associations
plot(all_tweets$tdm, term = findFreqTerms(all_tweets$tdm, lowfreq = 70), corThreshold = 0.1,
weighting = T, main="word association for all tweets")
#text classifier
#######
#we already have functions to build the document term matrix above so lets use those
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$X.2,15)
dtm<-dtm$dtm
#calculate distances
m  <- as.matrix(dtm)
m<-m[sample(nrow(m),size=2000,replace=FALSE),] #sample rows to prevent huge distance matrix
distMatrix <- dist(m, method="euclidean")
num_clusters <- 5
#perform hierachical clustering
hc <- hclust(distMatrix, "ward.D")
clustering <- cutree(hc, num_clusters)
test<-as.data.frame(cbind(m,clustering))
for (i in c(1:num_clusters)){
tryCatch({
cluster_dtm <- filter(test,clustering==as.character(i))%>%
select(-clustering)
m <- as.matrix(cluster_dtm)
v <- sort(colSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
make_wordcloud(d)
},
error = function(err){print("empty cluster")}
)
}
table(test$clustering)
#we already have functions to build the document term matrix above so lets use those
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$X.2,15)
dtm<-dtm$dtm
#calculate distances
dtm_m  <- as.matrix(dtm)
dtm_m<-dtm_m[sample(nrow(dtm_m),size=2000,replace=FALSE),] #sample rows to prevent huge distance matrix
distMatrix <- dist(dtm_m, method="euclidean")
num_clusters <- 5
#perform hierachical clustering
hc <- hclust(distMatrix, "ward.D")
clustering <- cutree(hc, num_clusters)
test<-as.data.frame(cbind(dtm_m,clustering))
for (i in c(1:num_clusters)){
tryCatch({
cluster_dtm <- filter(test,clustering==as.character(i))%>%
select(-clustering)
m <- as.matrix(cluster_dtm)
v <- sort(colSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
make_wordcloud(d)
},
error = function(err){print("empty cluster")}
)
}
table(test$clustering)
make_wordcloud(all_tweets$freq_tab)
warnings()
#function to make wordclouds
make_wordcloud <- function(freq_tab){
wordcloud(words = freq_tab$word, freq = freq_tab$freq, min.freq = 5,max.words=200, random.order=FALSE, rot.per=0.2,colors=brewer.pal(8, "Dark2"))
}
make_wordcloud(all_tweets$freq_tab)
all_tweets$freq_table
