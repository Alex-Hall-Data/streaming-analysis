results
container@classification_matrix
container@testing_codes
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
analytics
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
table(predicted=results$TREE_LABEL,actual=container@testing_codes)
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
container@training_matrix
View(container@training_matrix)
View(container@training_matrix@ra)
View(container@training_matrix@ja)
View(container@training_matrix@ia)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes, gamma=(10^(-5:-1) , cost=10^(-1:1)))
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes, gamma=10^(-5:-1) , cost=10^(-1:1))
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes, gamma=10^(-5:-1) , cost=10^(-1:1))
container@classification_matrix
container@testing_codes
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-5:-1) , cost=10^(-1:1))
tuned_svm$best.performance
tuned_svm$performances
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
#validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-5:-1) , cost=10^(-1:1))
tuned_svm$best.performance
library(e1071)
tuned_svm <- tune(method=randomForest,x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-5:-1) , cost=10^(-1:1))
library(e1071)
tuned_svm <- tune(method=randomForest,train.x=container@training_matrix, train.y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-5:-1) , cost=10^(-1:1))
?tune.svm
tuned_svm <- tune.randomForest(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-5:-1) , cost=10^(-1:1))
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-5:-1) , cost=10^(-1:1))
tuned_svm <- tune.randomForest(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
mtry=(1:50) , nodesize=(1:50))
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-5:-1) , cost=10^(-1:1))
dtm
dtm$i
?TermDocumentMatrix
dtm[1,]
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-7:-1) , cost=10^(-1:2))
tuned_svm$best.performance
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$text,5)
dtm<-dtm$dtm
#checknumber of columns used for model
ncol(as.matrix(dtm))
container <- create_container(dtm, as.numeric(factor(df$label)),trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
#train some models and se what we get
models <- train_models(container,algorithms=c("SVM","BAGGING","BOOSTING","RF"))
results <- classify_models(container, models) #can use for conf matrix
analytics <- create_analytics(container, results)
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-7:-1) , cost=10^(-1:2))
tuned_svm$best.performance
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$text,20)
dtm<-dtm$dtm
#checknumber of columns used for model
ncol(as.matrix(dtm))
container <- create_container(dtm, as.numeric(factor(df$label)),trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
#train some models and se what we get
models <- train_models(container,algorithms=c("SVM","BAGGING","BOOSTING","RF"))
results <- classify_models(container, models) #can use for conf matrix
analytics <- create_analytics(container, results)
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-7:-1) , cost=10^(-1:2))
tuned_svm$best.performance
plot(leave$tdm, term = findFreqTerms(leave$tdm, lowfreq = 1), corThreshold = 0.1,
weighting = T, main="word association for leave tweets")
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$text,1)
dtm<-dtm$dtm
#checknumber of columns used for model
ncol(as.matrix(dtm))
container <- create_container(dtm, as.numeric(factor(df$label)),trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
#train some models and se what we get
models <- train_models(container,algorithms=c("SVM","BAGGING","BOOSTING","RF"))
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
#checknumber of columns used for model
ncol(as.matrix(dtm))
container <- create_container(dtm, as.numeric(factor(df$label)),trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
#train some models and se what we get
models <- train_models(container,algorithms=c("SVM","BAGGING","BOOSTING","RF"))
results <- classify_models(container, models) #can use for conf matrix
analytics <- create_analytics(container, results)
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, as.numeric(factor(df$label)),trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-7:-1) , cost=10^(-1:2))
print('model accuracy = ' , 1 - tuned_svm$best.performance)
tuned_svm$best.performance
tuned_svm$best.performance[1]
1 - tuned_svm$best.performance
print('model accuracy = ' , (1 - tuned_svm$best.performance))
print('model accuracy = ' , tuned_svm$best.performance)
print('model accuracy = ' )
1-tuned_svm$best.performance
analytics
library(dplyr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(ggplot2)
library(gridExtra)
library(graph)
library(Rgraphviz)
library(RTextTools)
df <- read.csv("leaveRemainTweets_CW.csv")
#seperate dataframes for leave and remain tweets
leave_tweets <- df%>%
filter(label=="Leave")
remain_tweets <- df%>%
filter(label=="Remain")
#check we have captured all the tweets
nrow(df)==(nrow(leave_tweets) + nrow(remain_tweets))
nrow(leave_tweets)
nrow(remain_tweets)
buildCorpus <- function(someText){
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(someText))
myCorpus <- tm_map(myCorpus,
content_transformer(function(x) iconv(x, to='UTF-8',sub='byte')))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) {
gsub("http[[:alnum:]]*", "", x)
}
myCorpus <- tm_map(myCorpus, removeURL)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL)) #??
# add stopwords
myStopwords <- c(stopwords("english"), "RT","rt","EU","eu","brexit","euref")
#remove words CONTAINING certain terms
myCorpus <- tm_map(myCorpus, content_transformer(gsub), pattern = "*vote*|*leave*|*remain*|*stronger*|*referendum*", replacement = "")
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
#
myCorpus <- tm_map(myCorpus, stripWhitespace)
# Return the text corpus
return(myCorpus)
}
#function to build term document matrix, along with other useful objects for analysis
make_tdm <- function(text_column,minfreq=1){
text_column <- iconv(text_column, 'UTF-8', 'ASCII')
corpus <- buildCorpus(text_column)
# keep for later
corpus_copy <- corpus
# stem words
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus,control=list(bounds = list(global = c(minfreq,Inf))))
dtm <- DocumentTermMatrix(corpus,control=list(bounds = list(global = c(minfreq,Inf))))
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
return(list(corpus=corpus,freq_table=d,tdm=tdm , dtm=dtm))
}
#make corpuses and tdms from the tweets
all_tweets <- make_tdm(df$text)
remain <- make_tdm(remain_tweets$text)
leave <- make_tdm(leave_tweets$text)
#function to make wordclouds
make_wordcloud <- function(freq_tab){
wordcloud(words = freq_tab$word, freq = freq_tab$freq, min.freq = 5,max.words=2000, random.order=FALSE, rot.per=0.2,colors=brewer.pal(8, "Dark2"))
}
#make corpuses and tdms from the tweets
all_tweets <- make_tdm(df$text)
remain <- make_tdm(remain_tweets$text)
leave <- make_tdm(leave_tweets$text)
#function to make wordclouds
make_wordcloud <- function(freq_tab){
wordcloud(words = freq_tab$word, freq = freq_tab$freq, min.freq = 5,max.words=2000, random.order=FALSE, rot.per=0.2,colors=brewer.pal(8, "Dark2"))
}
#make wordclouds from the frequency tables
make_wordcloud(remain$freq_tab)
#function to plot wordcounts
plot_wordcount <- function(freq_tab,title_text){
ggplot(head(freq_tab,20), aes(x=word,y=freq))+
geom_bar(stat="identity")+
labs(title=title_text)+
theme(axis.text.x = element_text(angle = 30, hjust = 1))
}
a<-plot_wordcount(all_tweets$freq_table,"most common words for all tweets")
b<-plot_wordcount(remain$freq_table,"most common words for remain tweets")
c<-plot_wordcount(leave$freq_table,"most common words for leave tweets")
plot(remain$tdm, term = findFreqTerms(remain$tdm, lowfreq = 10), corThreshold = 0.1,
weighting = T, main="word association for remain tweets")
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$text,5)
dtm<-dtm$dtm
#checknumber of columns used for model
ncol(as.matrix(dtm))
container <- create_container(dtm, as.numeric(factor(df$label)),trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
#train some models and se what we get
models <- train_models(container,algorithms=c("SVM","BAGGING","BOOSTING","RF"))
results <- classify_models(container, models) #can use for conf matrix
analytics <- create_analytics(container, results)
analytics
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, as.numeric(factor(df$label)),trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-7:-1) , cost=10^(-1:2))
print('model accuracy = ' )
1-tuned_svm$best.performance
??train_models
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, factor(df$label),trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-7:-1) , cost=10^(-1:2))
print('model accuracy = ' )
1-tuned_svm$best.performance
str(df$label)
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-7:-1) , cost=10^(-1:2))
print('model accuracy = ' )
1-tuned_svm$best.performance
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$text,5)
dtm<-dtm$dtm
#checknumber of columns used for model
ncol(as.matrix(dtm))
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
#train some models and se what we get
models <- train_models(container,algorithms=c("SVM","BAGGING","BOOSTING","RF"))
results <- classify_models(container, models) #can use for conf matrix
analytics <- create_analytics(container, results)
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
library(dplyr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(ggplot2)
library(gridExtra)
library(graph)
library(Rgraphviz)
library(RTextTools)
df <- read.csv("leaveRemainTweets_CW.csv")
#seperate dataframes for leave and remain tweets
leave_tweets <- df%>%
filter(label=="Leave")
remain_tweets <- df%>%
filter(label=="Remain")
#check we have captured all the tweets
nrow(df)==(nrow(leave_tweets) + nrow(remain_tweets))
nrow(leave_tweets)
nrow(remain_tweets)
buildCorpus <- function(someText){
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(someText))
myCorpus <- tm_map(myCorpus,
content_transformer(function(x) iconv(x, to='UTF-8',sub='byte')))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) {
gsub("http[[:alnum:]]*", "", x)
}
myCorpus <- tm_map(myCorpus, removeURL)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL)) #??
# add stopwords
myStopwords <- c(stopwords("english"), "RT","rt","EU","eu","brexit","euref")
#remove words CONTAINING certain terms
myCorpus <- tm_map(myCorpus, content_transformer(gsub), pattern = "*vote*|*leave*|*remain*|*stronger*|*referendum*", replacement = "")
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
#
myCorpus <- tm_map(myCorpus, stripWhitespace)
# Return the text corpus
return(myCorpus)
}
#function to build term document matrix, along with other useful objects for analysis
make_tdm <- function(text_column,minfreq=1){
text_column <- iconv(text_column, 'UTF-8', 'ASCII')
corpus <- buildCorpus(text_column)
# keep for later
corpus_copy <- corpus
# stem words
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus,control=list(bounds = list(global = c(minfreq,Inf))))
dtm <- DocumentTermMatrix(corpus,control=list(bounds = list(global = c(minfreq,Inf))))
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
return(list(corpus=corpus,freq_table=d,tdm=tdm , dtm=dtm))
}
#make corpuses and tdms from the tweets
all_tweets <- make_tdm(df$text)
remain <- make_tdm(remain_tweets$text)
leave <- make_tdm(leave_tweets$text)
#function to make wordclouds
make_wordcloud <- function(freq_tab){
wordcloud(words = freq_tab$word, freq = freq_tab$freq, min.freq = 5,max.words=2000, random.order=FALSE, rot.per=0.2,colors=brewer.pal(8, "Dark2"))
}
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$text,5)
dtm<-dtm$dtm
#checknumber of columns used for model
ncol(as.matrix(dtm))
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
#train some models and se what we get
models <- train_models(container,algorithms=c("SVM","BAGGING","BOOSTING","RF"))
results <- classify_models(container, models) #can use for conf matrix
#analytics <- create_analytics(container, results)
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-7:-1) , cost=10^(-1:2))
print('model accuracy = ' )
1-tuned_svm$best.performance
tuned_svm$method
tuned_svm$performances
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-3:0) , cost=10^(-1:3))
print('model accuracy = ' )
1-tuned_svm$best.performance
tuned_svm$performances
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=FALSE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-3:0) , cost=10^(-1:5))
print('model accuracy = ' )
1-tuned_svm$best.performance
?create_container
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=TRUE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-3:0) , cost=10^(-1:3))
print('model accuracy = ' )
1-tuned_svm$best.performance
#transpose the dtm to get word count for each tweet
dtm<-make_tdm(df$text,10)
dtm<-dtm$dtm
#checknumber of columns used for model
ncol(as.matrix(dtm))
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=TRUE)
#train some models and se what we get
models <- train_models(container,algorithms=c("SVM","BAGGING","BOOSTING","RF"))
results <- classify_models(container, models) #can use for conf matrix
#analytics <- create_analytics(container, results)
print("SVM")
table(predicted=results$SVM_LABEL,actual=container@testing_codes)
print("Bagging")
table(predicted=results$BAGGING_LABEL,actual=container@testing_codes)
print("Boosting")
table(predicted=results$LOGITBOOST_LABEL,actual=container@testing_codes)
print("Random Forest")
table(predicted=results$FORESTS_LABEL,actual=container@testing_codes)
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=TRUE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-3:0) , cost=10^(-1:3))
print('model accuracy = ' )
1-tuned_svm$best.performance
dtm<-make_tdm(df$text,2)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=TRUE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-6:0) , cost=10^(-1:3))
print('model accuracy = ' )
1-tuned_svm$best.performance
container@training_matrix
nrow(leave_tweets)
nrow(remain_tweets)
dtm<-make_tdm(df$text,1)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=TRUE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-6:0) , cost=10^(-1:3))
print('model accuracy = ' )
1-tuned_svm$best.performance
dtm<-make_tdm(df$text,10)
dtm<-dtm$dtm
container <- create_container(dtm, df$label,trainSize=1:round(0.7*nrow(df)), testSize=(round(0.7*nrow(df))+1):nrow(df),virgin=TRUE)
library(e1071)
tuned_svm <- tune.svm(x=container@training_matrix, y=container@training_codes,
validation.x = container@classification_matrix, validation.y=container@testing_codes,
gamma=10^(-6:0) , cost=10^(-1:3))
print('model accuracy = ' )
1-tuned_svm$best.performance
